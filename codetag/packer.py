"""
Core logic for the 'pack' command.

This module provides `pack_repository`, a reusable function that encapsulates the
full packing algorithm previously implemented directly inside `cli.py`. The
function selects the most valuable text files in a repository while respecting
an overall token budget so that the resulting context file can be fed to an
LLM.
"""

from __future__ import annotations

import json
import time
from pathlib import Path
from typing import List, Dict, Any, Set, Optional

import tiktoken

# In-package helpers ---------------------------------------------------------
from .fs_tree import build_fs_tree, flatten_fs_tree
from .metrics import analyze_python_file_metrics
# _get_all_files_from_tree superseded by flatten_fs_tree

# Marker used to detect self-generated files (copied from cli)
CODETAG_MARKER = "# -- GENERATED BY CODETAG --"

# Black-listed extensions (hard-coded)
BINARY_EXTENSIONS = {
    ".npz", ".npy", ".pt", ".pth", ".bin", ".onnx", ".safetensors", ".pkl", ".joblib",
    ".data", ".dat", ".dll", ".so", ".exe", ".o", ".a", ".png", ".jpg", ".jpeg", ".gif",
    ".svg", ".ico", ".webp", ".mp3", ".mp4", ".wav", ".mov", ".avi", ".zip", ".gz", ".tar",
    ".rar", ".7z", ".pdf", ".pyc", ".pyo",
}

# --- Constants ---
HEADER_TOKENS_PER_FILE = 50  # heuristic overhead per file

# ---------------------------------------------------------------------------
# Internal Helpers for pack_repository
# ---------------------------------------------------------------------------

# Local variable removed; use constant

def _gather_candidates(
    all_files: List[Path],
    *,
    blacklist: Set[str],
    max_bytes: int,
    max_tokens: int,
    output_file: Path,
    encoding: tiktoken.Encoding,
) -> List[Dict[str, Any]]:
    """
    First pass to filter and score all files in the repository.

    Returns a list of candidate file dictionaries, each containing path, content,
    cost, value, and density.
    """
    candidates: List[Dict[str, Any]] = []

    for fp in all_files:
        # Basic filters
        if fp.is_symlink() or fp.suffix.lower() in blacklist:
            continue
        try:
            if fp.stat().st_size == 0 or fp.stat().st_size > max_bytes:
                continue
        except (FileNotFoundError, OSError):
            continue

        # Skip files previously generated by CodeTag
        try:
            with fp.open("r", encoding="utf-8", errors="ignore") as marker_f:
                if CODETAG_MARKER in marker_f.readline():
                    continue
        except Exception:
            pass

        # Read content and perform final checks
        try:
            content = fp.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            continue
        if "\x00" in content:  # binary-like
            continue

        cost_tokens = len(encoding.encode(content))
        if cost_tokens == 0 or cost_tokens + HEADER_TOKENS_PER_FILE > max_tokens:
            continue

        # VALUE heuristic
        value = len(content.splitlines())
        if fp.suffix.lower() == ".py":
            metrics = analyze_python_file_metrics(fp)
            if metrics:
                value += metrics.get("total_complexity", 0) * 10
                value += metrics.get("function_count", 0) * 5

        if value == 0:
            continue

        candidates.append({
            "path": fp,
            "content": content,
            "cost": cost_tokens,
            "value": value,
            "density": value / cost_tokens,
        })
    return candidates


# ---------------------------------------------------------------------------
def _run_knapsack_selection(
    candidates: List[Dict[str, Any]],
    *,
    max_tokens: int,
) -> List[Dict[str, Any]]:
    """
    Selects the best files using a greedy knapsack algorithm based on value density.
    """
    # Sort candidates by value density (value per token)
    candidates.sort(key=lambda c: c["density"], reverse=True)

    selected: List[Dict[str, Any]] = []
    token_tally = 0
    for cand in candidates:
        total_cost = cand["cost"] + HEADER_TOKENS_PER_FILE
        if token_tally + total_cost <= max_tokens:
            selected.append(cand)
            token_tally += total_cost

    # If nothing fits, check if the single smallest file can fit.
    if not selected:
        smallest = min(candidates, key=lambda c: c["cost"])
        if smallest["cost"] + HEADER_TOKENS_PER_FILE <= max_tokens:
            selected.append(smallest)

    return selected


def _format_packed_output(
    selected_files: List[Dict[str, Any]],
    *,
    output_format: str,
    root_path: Path,
) -> str:
    """Constructs the final output string in either 'raw' or 'json' format."""
    if output_format.lower() == "json":
        files_payload = [
            {"path": str(c["path"].relative_to(root_path)), "content": c["content"]}
            for c in selected_files
        ]
        output_body = json.dumps(
            {
                "pack_date": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                "files": files_payload,
            },
            indent=2,
        )
    elif output_format.lower() == "raw":
        sep = "=" * 42
        parts: List[str] = []
        for c in selected_files:
            parts.append(sep)
            parts.append(f"FILE: {c['path'].relative_to(root_path)}")
            parts.append(sep)
            parts.append(c["content"])
            parts.append("")
        output_body = "\n".join(parts)
    else:
        raise ValueError(f"Invalid output_format: {output_format}. Use 'raw' or 'json'.")

    return f"{CODETAG_MARKER}\n\n{output_body}"


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------

def pack_repository(
    *,
    path: Path,
    output_file: Path,
    max_tokens: int = 250_000,
    max_file_size_kb: int = 100,
    exclude_extensions: Optional[Set[str]] = None,
    output_format: str = "raw",
) -> Dict[str, Any]:
    """Pack a repository into a single context file.

    Parameters
    ----------
    path:
        Root directory of the repository to pack.
    output_file:
        Where the packed file will be written.
    max_tokens:
        Hard token budget for *final* output (includes separators & metadata).
    max_file_size_kb:
        Skip any individual file larger than this many kilobytes.
    exclude_extensions:
        Additional file extensions (e.g. {".log", ".csv"}) to ignore.
        Extensions are compared in lower-case and *must* include the leading dot.
    output_format:
        Either ``"raw"`` or ``"json"`` â€“ matches the original CLI semantics.

    Returns
    -------
    dict
        Simple statistics about the pack operation (files selected, token
        count, duration_seconds).
    """

    start_time = time.time()

    # 1. Initialization
    try:
        encoding = tiktoken.get_encoding("cl100k_base")
    except Exception as exc:
        raise RuntimeError("Could not initialise tiktoken tokenizer.") from exc

    # 2. Gather and filter all files from the tree
    tree = build_fs_tree(path, include_hidden=False)
    all_files = flatten_fs_tree(tree, prefix=path)
    absolute_output = output_file.resolve()
    all_files = [f for f in all_files if f.resolve() != absolute_output]

    # 3. Gather candidates
    blacklist = BINARY_EXTENSIONS.union(exclude_extensions or set())
    candidates = _gather_candidates(
        all_files,
        blacklist=blacklist,
        max_bytes=max_file_size_kb * 1024,
        max_tokens=max_tokens,
        output_file=output_file,
        encoding=encoding,
    )
    if not candidates:
        raise ValueError("No eligible text files found after filtering.")

    # 4. Select best files using knapsack algorithm
    selected = _run_knapsack_selection(candidates, max_tokens=max_tokens)
    if not selected:
        raise ValueError("No single file can fit inside the given token budget.")

    # 5. Format the output
    final_output_string = _format_packed_output(
        selected, output_format=output_format, root_path=path
    )
    final_token_count = len(encoding.encode(final_output_string))

    # 6. Write file
    output_file.parent.mkdir(parents=True, exist_ok=True)
    output_file.write_text(final_output_string, encoding="utf-8")

    return {
        "files_selected": len(selected),
        "final_token_count": final_token_count,
        "duration_seconds": round(time.time() - start_time, 2),
    } 